{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cde04f4-0377-4601-b2bd-62fd16edb82c",
   "metadata": {},
   "source": [
    "Author: Dorys Trujillo  \n",
    "Project: Legal Uncertainty Index (IIJ)  \n",
    "Data Source: Ministry of Commerce, Industry and Tourism (MinCIT)  \n",
    "Period: 2018–2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c8c2e-208e-4c3f-8223-efb601423f89",
   "metadata": {},
   "source": [
    "## Automated Web Scraping of MinCIT Regulatory Drafts (2018–2025)\n",
    "\n",
    "This notebook implements an automated pipeline to collect draft decree documents published by the Colombian Ministry of Commerce, Industry and Tourism (MinCIT) between 2018 and 2025. The extracted corpus serves as the primary data source for constructing the Legal Uncertainty Index (IIJ) and represents the data ingestion layer of the project.\n",
    "\n",
    "The workflow includes automated navigation, link extraction, file downloads, and structured local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c8111-7705-4b60-a3cc-b90fb4f5f9f3",
   "metadata": {},
   "source": [
    "### Environment Setup and Dependencies\n",
    "\n",
    "This step installs the required dependencies for the scraping pipeline, including Playwright (browser automation), Requests (HTTP downloads), nest_asyncio (Jupyter event loop support), and the Chromium browser engine.\n",
    "\n",
    "⚠️ These commands only need to be executed once per environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1db9467-a18a-433d-b3d0-c58fdbd32565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time environment setup (run only once per environment)\n",
    "\n",
    "# Install required Python packages\n",
    "# pip install playwright requests nest_asyncio\n",
    "\n",
    "# Install Playwright browser dependencies\n",
    "# playwright install chromium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fdb3d-c057-4cbd-8f9c-8d90b6079c77",
   "metadata": {},
   "source": [
    "### Library Imports and Core Dependencies\n",
    "\n",
    "This section loads all core libraries required for the scraping pipeline, covering file system operations, asynchronous execution, HTTP communication, URL processing, Playwright-based browser automation, and tabular data management with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c31f76-4bcb-4587-90e4-0e7b1aea3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and file management\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Time and asynchronous execution\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# Text processing and pattern matching\n",
    "import re\n",
    "\n",
    "# HTTP requests and file downloads\n",
    "import requests\n",
    "\n",
    "# URL utilities for decoding and link reconstruction\n",
    "from urllib.parse import unquote, urljoin\n",
    "\n",
    "# Browser automation for dynamic web scraping\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Data handling and tabular storage\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3b6a5-3771-48f1-b341-5a28dd2ca842",
   "metadata": {},
   "source": [
    "### Windows Compatibility Fix\n",
    "\n",
    "This section applies a Windows-specific event loop policy required to prevent Playwright and AsyncIO conflicts in Jupyter environments. Deprecation warnings are also suppressed to keep execution output clean. The configuration only affects Windows systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e8d943-51ad-4487-8cbc-680ba52756f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows compatibility fix for Playwright\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "# Suppress deprecation warnings to reduce console noise\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Apply Windows-specific event loop policy required by Playwright\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9b6b3-5c69-4486-bb48-f0ad74265cb1",
   "metadata": {},
   "source": [
    "### Async Execution Helper (Jupyter + Playwright)\n",
    "\n",
    "This helper enables safe execution of asynchronous Playwright tasks inside Jupyter notebooks by running coroutines in an isolated background thread with a dedicated event loop. On Windows, a Proactor event loop policy is applied to ensure proper subprocess handling.\n",
    "\n",
    "This approach improves stability, compatibility, and error isolation when running browser automation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6e23f8-9afa-4710-aa85-2daa1a87d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to safely run async Playwright coroutines inside Jupyter\n",
    "\n",
    "def run_coro_in_thread(coro):\n",
    "    import threading\n",
    "    import queue\n",
    "\n",
    "    # Suppress deprecation warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    # Thread-safe queue to retrieve execution results\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def _runner():\n",
    "        try:\n",
    "            # Apply Windows-specific Proactor event loop policy for subprocess support\n",
    "            if sys.platform.startswith(\"win\"):\n",
    "                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "\n",
    "            # Create and assign a new isolated event loop\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "\n",
    "            try:\n",
    "                result = loop.run_until_complete(coro)\n",
    "                q.put((\"ok\", result))\n",
    "            finally:\n",
    "                loop.close()\n",
    "\n",
    "        except BaseException as e:\n",
    "            q.put((\"err\", e))\n",
    "\n",
    "    # Run coroutine in background thread\n",
    "    t = threading.Thread(target=_runner, daemon=True)\n",
    "    t.start()\n",
    "\n",
    "    # Retrieve execution result\n",
    "    kind, val = q.get()\n",
    "\n",
    "    if kind == \"err\":\n",
    "        raise val\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3119b4-a1eb-43a1-89fa-94bc37603b52",
   "metadata": {},
   "source": [
    "### Scraping Configuration and Execution Overview\n",
    "\n",
    "This section defines the main scraping parameters and execution workflow for collecting MinCIT draft decree documents (2018–2025).\n",
    "\n",
    "### Key Features\n",
    "- Year-based navigation with multiple URL fallbacks per period.\n",
    "- Automated browser-based downloads using Playwright.\n",
    "- Polite crawling strategy with navigation and download delays.\n",
    "- Robust error handling with retry and backoff logic.\n",
    "- Automatic generation of a CSV manifest to track downloaded files and metadata.\n",
    "\n",
    "### Output\n",
    "- Downloaded documents are stored locally in year-specific folders.\n",
    "- A cumulative `downloads_manifest.csv` file is created to support traceability and reproducibility.\n",
    "\n",
    "### Execution\n",
    "The pipeline is executed asynchronously through Playwright and launched inside Jupyter using a dedicated event loop wrapper to ensure platform compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd75fba-9a36-442f-9aeb-2a52ef568f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== DOWNLOAD CONFIGURATION ==================\n",
    "\n",
    "TARGET_YEARS = range(2017, 2026)  # Includes 2017 due to site structure; target period is 2018–2025\n",
    "BASE_PREFIX = \"https://www.mincit.gov.co/normatividad/proyectos-de-normatividad\"\n",
    "\n",
    "# Local folder where downloaded documents will be stored\n",
    "DOWNLOAD_FOLDER = \"Documentos_MINCIT\"\n",
    "os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "# Navigation and download pacing (polite scraping)\n",
    "NAV_DELAY_SEC = 1.2              # Delay between page navigations\n",
    "DL_DELAY_SEC = 1.0               # Delay between individual downloads\n",
    "\n",
    "# Download robustness parameters\n",
    "MAX_CONCURRENT_DOWNLOADS = 3     # Maximum concurrent downloads (reserved for future use)\n",
    "MAX_RETRIES = 3                  # Number of retry attempts per download\n",
    "BACKOFF_BASE = 1.8               # Exponential backoff factor\n",
    "REQUEST_TIMEOUT = 120            # Request timeout (seconds)\n",
    "CHUNK_SIZE = 1024 * 256          # Streaming chunk size (256 KB)\n",
    "VERIFY_SSL = True                # Keep enabled in production environments\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "def candidate_year_urls(year: int):\n",
    "    \"\"\"\n",
    "    Generate candidate landing page URLs for a given year.\n",
    "    The MinCIT website uses different naming patterns (singular/plural and historical paths).\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f\"{BASE_PREFIX}/proyectos-de-decreto-{year}\",\n",
    "        f\"{BASE_PREFIX}/proyectos-de-decretos-{year}\",\n",
    "        f\"{BASE_PREFIX}/proyecto-de-decretos-{year}\",\n",
    "        f\"{BASE_PREFIX}/historial-proyectos/proyectos-de-decreto-{year}\",\n",
    "        f\"{BASE_PREFIX}/historial-proyectos/proyectos-de-decretos-{year}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove invalid filesystem characters to ensure cross-platform compatibility.\n",
    "    \"\"\"\n",
    "    name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n",
    "    return name.strip()\n",
    "\n",
    "def filename_from_cd(cd_header: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract filename from the Content-Disposition HTTP header, if available.\n",
    "    \"\"\"\n",
    "    if not cd_header:\n",
    "        return None\n",
    "\n",
    "    m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\\\";]+)\"?', cd_header, re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    return sanitize_filename(unquote(m.group(1)))\n",
    "\n",
    "def is_pdf_response(resp: requests.Response) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether the HTTP response corresponds to a PDF file.\n",
    "    \"\"\"\n",
    "    ct = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return \"application/pdf\" in ct or ct.endswith(\"/pdf\")\n",
    "\n",
    "async def polite_sleep(seconds: float):\n",
    "    \"\"\"\n",
    "    Introduce controlled delays to reduce server load and avoid aggressive scraping.\n",
    "    \"\"\"\n",
    "    await asyncio.sleep(seconds)\n",
    "\n",
    "async def download_pdf(url: str, out_path: str, session: requests.Session) -> dict | None:\n",
    "    \"\"\"\n",
    "    Download a PDF file using HTTP streaming and return metadata.\n",
    "    Returns None if the download fails.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            with session.get(\n",
    "                url,\n",
    "                timeout=REQUEST_TIMEOUT,\n",
    "                stream=True,\n",
    "                verify=VERIFY_SSL\n",
    "            ) as r:\n",
    "\n",
    "                r.raise_for_status()\n",
    "\n",
    "                # Validate PDF content type (fallback: allow .pdf extension)\n",
    "                if not is_pdf_response(r) and not url.lower().endswith(\".pdf\"):\n",
    "                    print(f\"    ! Non-PDF content detected (Content-Type={r.headers.get('Content-Type')}), skipping.\")\n",
    "                    return None\n",
    "\n",
    "                cd_name = filename_from_cd(r.headers.get(\"Content-Disposition\", \"\"))\n",
    "                final_path = os.path.join(os.path.dirname(out_path), cd_name) if cd_name else out_path\n",
    "\n",
    "                total = 0\n",
    "                with open(final_path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            total += len(chunk)\n",
    "\n",
    "                size_mb = total / (1024 * 1024)\n",
    "                print(f\"    ✓ Saved {os.path.basename(final_path)} ({size_mb:.2f} MB)\")\n",
    "\n",
    "                return {\n",
    "                    \"saved_path\": os.path.abspath(final_path),\n",
    "                    \"bytes\": total,\n",
    "                    \"filename\": os.path.basename(final_path),\n",
    "                    \"source_url\": url,\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                wait = (BACKOFF_BASE ** (attempt - 1))\n",
    "                print(f\"    ! Attempt {attempt}/{MAX_RETRIES} failed: {e} -> retrying in {wait:.1f}s\")\n",
    "                await asyncio.sleep(wait)\n",
    "            else:\n",
    "                print(f\"    ✗ Download failed after {MAX_RETRIES} attempts: {e}\")\n",
    "                return None\n",
    "\n",
    "async def scrape_year(page, year: int) -> int:\n",
    "    \"\"\"\n",
    "    Scrape all available draft decree documents for a given year.\n",
    "    Returns the number of successfully downloaded files.\n",
    "    \"\"\"\n",
    "    downloaded = 0\n",
    "    manifest_rows = []  # Accumulate metadata rows for the current year\n",
    "\n",
    "    for url in candidate_year_urls(year):\n",
    "        try:\n",
    "            await page.goto(url, timeout=90000, wait_until=\"load\")\n",
    "            await polite_sleep(NAV_DELAY_SEC)\n",
    "\n",
    "            # Scroll to trigger lazy-loaded content\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await polite_sleep(0.6)\n",
    "\n",
    "            # Primary selector for \"Ver documento\" links\n",
    "            anchors = await page.get_by_role(\n",
    "                \"link\",\n",
    "                name=re.compile(r\"ver\\s*documento\", re.I)\n",
    "            ).all()\n",
    "\n",
    "            if not anchors:\n",
    "                anchors = await page.locator('a:has-text(\"Ver documento\")').all()\n",
    "\n",
    "            if not anchors:\n",
    "                # Fallback selector for any link/button mentioning \"documento\"\n",
    "                anchors = await page.locator(\n",
    "                    'a, button, [role=\"link\"]',\n",
    "                    has_text=re.compile(r\"documento\", re.I)\n",
    "                ).all()\n",
    "\n",
    "            if not anchors:\n",
    "                print(f\"  -> {year}: no 'Ver documento' links found at {url}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  -> {year}: {len(anchors)} documents found at {url}\")\n",
    "\n",
    "            year_folder = os.path.join(DOWNLOAD_FOLDER, f\"PDecretos {year}\")\n",
    "            os.makedirs(year_folder, exist_ok=True)\n",
    "\n",
    "            # Sequential downloads (human-like behavior)\n",
    "            for i, a in enumerate(anchors, start=1):\n",
    "                await polite_sleep(DL_DELAY_SEC)\n",
    "\n",
    "                try:\n",
    "                    # Wait for browser download event triggered by click\n",
    "                    async with page.expect_download(timeout=60000) as dl_info:\n",
    "                        await a.click(button=\"left\")\n",
    "\n",
    "                    download = await dl_info.value\n",
    "\n",
    "                    suggested = download.suggested_filename or f\"Decreto_{i}.pdf\"\n",
    "                    fname = sanitize_filename(suggested)\n",
    "                    final_path = os.path.join(year_folder, fname)\n",
    "\n",
    "                    await download.save_as(final_path)\n",
    "\n",
    "                    size_bytes = os.path.getsize(final_path)\n",
    "                    downloaded += 1\n",
    "\n",
    "                    print(f\"    ✓ Saved {fname} ({size_bytes / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "                    manifest_rows.append({\n",
    "                        \"saved_path\": os.path.abspath(final_path),\n",
    "                        \"bytes\": size_bytes,\n",
    "                        \"filename\": fname,\n",
    "                        \"source_url\": download.url or \"\",\n",
    "                        \"year\": int(year),\n",
    "                        \"landing_page\": url,\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    ! Download failed for link #{i}: {e}\")\n",
    "\n",
    "            # Append yearly batch to manifest CSV\n",
    "            if manifest_rows:\n",
    "                df = pd.DataFrame(manifest_rows)\n",
    "\n",
    "                csv_path = os.path.join(DOWNLOAD_FOLDER, \"downloads_manifest.csv\")\n",
    "                header = not os.path.exists(csv_path)\n",
    "\n",
    "                df.to_csv(\n",
    "                    csv_path,\n",
    "                    mode=\"a\",\n",
    "                    header=header,\n",
    "                    index=False,\n",
    "                    encoding=\"utf-8\"\n",
    "                )\n",
    "\n",
    "                print(f\"  -> Manifest updated: {csv_path} (+{len(df)} rows)\")\n",
    "\n",
    "            # Stop searching alternative URLs if this one succeeded\n",
    "            return downloaded\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> {year}: error while processing {url}: {e}\")\n",
    "\n",
    "    print(f\"  -> {year}: no valid landing page found.\")\n",
    "    return downloaded\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main orchestration function that launches the browser,\n",
    "    iterates over target years, and aggregates download statistics.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "\n",
    "        context = await browser.new_context(\n",
    "            user_agent=(\n",
    "                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\"\n",
    "            ),\n",
    "            ignore_https_errors=True\n",
    "        )\n",
    "\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for year in TARGET_YEARS:\n",
    "            print(f\"\\n=== YEAR {year} ===\")\n",
    "            count = await scrape_year(page, year)\n",
    "            total += count\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(\"\\n--- PROCESS COMPLETED ---\")\n",
    "    print(f\"Total documents downloaded: {total}\")\n",
    "    print(f\"Output folder: {DOWNLOAD_FOLDER}\")\n",
    "\n",
    "# Entry point for execution inside Jupyter/Colab environments\n",
    "# await main()\n",
    "run_coro_in_thread(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
